{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "competent-victor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from random import seed, randrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sacrebleu\n",
    "import bert_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from models.LSTMBahdanau import Encoder, Decoder, Seq2Seq\n",
    "# from models.BiLSTMLuong import Encoder, Decoder, Seq2Seq\n",
    "from utils.tokenizer import Tokenizer, pad_sequences, respond\n",
    "from utils.tokenizer import respond_only_lstm_attn, respond_only_lstm_no_attn, respond_only_gru_no_attn, respond_only_gru_attn, respond_only_lstm_attn_reg\n",
    "from utils.preprocess import preprocess_1, preprocess_2\n",
    "# from trainer import train, loss_function, sort_within_batch\n",
    "from utils.evaluate import calculate_rouge, calculate_bertscore, calculate_bleu\n",
    "\n",
    "root_dir = '/home/alfirsafauzulh@student.ub.ac.id/Firsa/Research/Chatbot'\n",
    "\n",
    "data_dir = root_dir + '/Datasets'\n",
    "dailydialogs_root_dir = data_dir + '/dailydialog'\n",
    "cornell_root_dir = data_dir + '/cornell_movie'\n",
    "# saved_model_path = '/home/alfirsafauzulh@student.ub.ac.id/Firsa/Research/Chatbot/Code/autogen-chatbot-v2/saved_models/siet/LSTMSA-dailydialog-50-SbertFreeze'\n",
    "saved_model_path = '/home/alfirsafauzulh/Firsa/autogen-chatbot-v2/saved_models/siet/LSTMBahdanau-dailydialog-50-SbertFreeze'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        # TODO: convert this into torch code is possible\n",
    "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        x_len = self.length[index]\n",
    "        return x,y,x_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  \n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
    "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
    "    #print(mask)\n",
    "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    loss_ = criterion(pred, real) * mask \n",
    "    return torch.mean(loss_)\n",
    "\n",
    "### sort batch function to be able to use with pad_packed_sequence\n",
    "def sort_within_batch(X, y, lengths):\n",
    "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
    "    X = X[indx]\n",
    "    y = y[indx]\n",
    "    return X, y, lengths # transpose (batch x seq) to (seq x batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opened-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "th = 50\n",
    "# df = pd.read_csv(dailydialogs_root_dir + f'/df_dailydialogs_max_{th}.csv')\n",
    "df = pd.read_csv(f'./Datasets/dailydialog/df_dailydialogs_max_{th}.csv')\n",
    "# df = pd.read_csv(f'./Datasets/cornell_movie/df_cornell_max_{th}.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reduced-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(saved_model_path + \"/tokenizer.pickle\", 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-record",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data \t: 79684\n",
      "Test Data\t: 8854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = th+2\n",
    "\n",
    "df['questions_preprocessed'] = df['questions'].apply(preprocess_1)\n",
    "df['answers_preprocessed'] = df['answers'].apply(preprocess_1)\n",
    "\n",
    "df['questions_preprocessed'] = df['questions'].map(lambda x: preprocess_2(x))\n",
    "df['answers_preprocessed'] = df['answers'].map(lambda x: preprocess_2(x))\n",
    "\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=.1, random_state=RANDOM_SEED)\n",
    "# df_train, df_val = train_test_split(df_train, test_size=.25, random_state=RANDOM_SEED)\n",
    "    \n",
    "print(f\"Train Data \\t: {len(df_train)}\")\n",
    "# print(f\"Val Data \\t: {len(df_val)}\")\n",
    "print(f\"Test Data\\t: {len(df_test)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "british-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.LSTMBahdanau import Encoder, Decoder, Seq2Seq\n",
    "from models.LSTMBahdanauImproved import Encoder, Decoder, Seq2Seq\n",
    "# from models.LSTMSelfAttn import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "changed-thermal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(17177, 384)\n",
       "    (rnn): LSTM(384, 768, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(17177, 384)\n",
       "    (rnn): LSTM(1152, 768, dropout=0.5)\n",
       "    (fc): Linear(in_features=768, out_features=17177, bias=True)\n",
       "    (W1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (W2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (V): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size_encoder = len(tokenizer.vocab)+4\n",
    "input_size_decoder = len(tokenizer.vocab)+4\n",
    "output_size = len(tokenizer.vocab)+4\n",
    "vocab_len = len(tokenizer.vocab)+4\n",
    "\n",
    "# encoder_embedding_size = pretrained_word_embedding_dimensions\n",
    "# decoder_embedding_size = pretrained_word_embedding_dimensions\n",
    "\n",
    "encoder_embedding_size = 384\n",
    "decoder_embedding_size = 384\n",
    "\n",
    "hidden_size = 768\n",
    "batch_size = 256\n",
    "num_layers = 1\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "input_tensor_test = df_test['questions_preprocessed'].values.tolist()\n",
    "target_tensor_test = df_test['answers_preprocessed'].values.tolist()\n",
    "\n",
    "test_data = MyData(input_tensor_test, target_tensor_test)\n",
    "test_dataset = DataLoader(test_data, batch_size = batch_size, drop_last=True, shuffle=True)\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, \n",
    "                  num_layers, enc_dropout, pretrained_word_embedding=False, embedding_matrix=None, freeze=False).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, \n",
    "                      output_size, num_layers, dec_dropout, pretrained_word_embedding=False, embedding_matrix=None, freeze=False).to(device)\n",
    "    \n",
    "model = Seq2Seq(encoder_net, decoder_net, vocab_len=vocab_len)\n",
    "# model.load_state_dict(torch.load(saved_model_path + \"/best_loss.pth\", map_location=device))\n",
    "model.load_state_dict(torch.load(saved_model_path + \"/model.pth\", map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crucial-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.5345655679702759\n"
     ]
    }
   ],
   "source": [
    "pad_idx = tokenizer.word2index[\"<PAD>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  test_batch_loss = 0\n",
    "  test_num_batch = 0\n",
    "  for (test_batch_idx, (X_test, y_test, input_len)) in enumerate(test_dataset):\n",
    "    X, y, input_lengths = sort_within_batch(X_test, y_test, input_len)\n",
    "\n",
    "    X = X.permute(1,0)\n",
    "    y = y.permute(1,0)\n",
    "\n",
    "    test_inp_data = X.to(device)\n",
    "    test_target = y.to(device)\n",
    "\n",
    "    output, _ = model(test_inp_data, test_target, input_lengths)\n",
    "#     output = model(test_inp_data, test_target, input_lengths)\n",
    "      \n",
    "    output = output[1:].reshape(-1, output.shape[2])\n",
    "    test_target = test_target[1:].reshape(-1)\n",
    "\n",
    "    test_loss = loss_function(test_target, output)\n",
    "    test_batch_loss += test_loss\n",
    "    test_num_batch+=1\n",
    "\n",
    "  test_loss_ = test_batch_loss/test_num_batch\n",
    "\n",
    "  print(f\"test_loss: {test_loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "miniature-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = df_test['questions'].values\n",
    "test_answers = df_test['answers'].values\n",
    "\n",
    "preds = []\n",
    "for x in test_questions:\n",
    "#     preds.append(respond_only_lstm_no_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "#     preds.append(respond_only_lstm_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "    preds.append(respond_only_lstm_attn_reg(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "#     preds.append(respond_only_gru_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "#     preds.append(respond_only_gru_no_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "romantic-salem",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1-gram': 0.2856681956292383,\n",
       " '2-gram': 0.2111184715770366,\n",
       " '3-gram': 0.18562610581289485,\n",
       " '4-gram': 0.17685473167095725}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bleu(preds, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fewer-celtic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 0.2700691819190979, 'r': 0.26914358139038086, 'f': 0.2695634067058563}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bertscore(preds, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acute-french",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is james'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respond_only_lstm_attn_reg(model, str(\"What is your name ?\"), tokenizer, tokenizer, device, max_length=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-pepper",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_chatbot",
   "language": "python",
   "name": "thesis_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
