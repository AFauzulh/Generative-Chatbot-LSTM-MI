{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "competent-victor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from random import seed, randrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sacrebleu\n",
    "import bert_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from models.LSTMBahdanau import Encoder, Decoder, Seq2Seq\n",
    "# from models.BiLSTMLuong import Encoder, Decoder, Seq2Seq\n",
    "from utils.tokenizer import Tokenizer, pad_sequences, respond\n",
    "from utils.tokenizer import respond_only_lstm_attn, respond_only_lstm_no_attn, respond_only_gru_no_attn, respond_only_gru_attn, respond_only_lstm_attn_reg, respond_only_lstm_attn_reg_lm\n",
    "from utils.preprocess import preprocess_1, preprocess_2\n",
    "# from trainer import train, loss_function, sort_within_batch\n",
    "from utils.evaluate import calculate_rouge, calculate_bertscore, calculate_bleu\n",
    "\n",
    "root_dir = '/home/alfirsafauzulh@student.ub.ac.id/Firsa/Research/Chatbot'\n",
    "\n",
    "data_dir = root_dir + '/Datasets'\n",
    "dailydialogs_root_dir = data_dir + '/dailydialog'\n",
    "cornell_root_dir = data_dir + '/cornell_movie'\n",
    "# saved_model_path = '/home/alfirsafauzulh@student.ub.ac.id/Firsa/Research/Chatbot/Code/autogen-chatbot-v2/saved_models/siet/LSTMSA-dailydialog-50-SbertFreeze'\n",
    "saved_model_path = '/home/alfirsafauzulh/Firsa/autogen-chatbot-v2/saved_models/siet/LSTMBahdanauLM-dailydialog-100'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        # TODO: convert this into torch code is possible\n",
    "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        x_len = self.length[index]\n",
    "        return x,y,x_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  \n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
    "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
    "    #print(mask)\n",
    "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    loss_ = criterion(pred, real) * mask \n",
    "    return torch.mean(loss_)\n",
    "\n",
    "### sort batch function to be able to use with pad_packed_sequence\n",
    "def sort_within_batch(X, y, lengths):\n",
    "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
    "    X = X[indx]\n",
    "    y = y[indx]\n",
    "    return X, y, lengths # transpose (batch x seq) to (seq x batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opened-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "th = 100\n",
    "# df = pd.read_csv(dailydialogs_root_dir + f'/df_dailydialogs_max_{th}.csv')\n",
    "# df = pd.read_csv(f'./Datasets/dailydialog/df_dailydialogs_max_{th}.csv')\n",
    "df = pd.read_csv(f'./Datasets/dailydialog/df_dailydialogs_factory.csv')\n",
    "# df = pd.read_csv(f'./Datasets/cornell_movie/df_cornell_max_{th}.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reduced-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(saved_model_path + \"/tokenizer.pickle\", 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-record",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data \t: 80875\n",
      "Test Data\t: 8987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = th+2\n",
    "\n",
    "df['questions_preprocessed'] = df['questions'].apply(preprocess_1)\n",
    "df['answers_preprocessed'] = df['answers'].apply(preprocess_1)\n",
    "\n",
    "df['questions_preprocessed'] = df['questions'].map(lambda x: preprocess_2(x))\n",
    "df['answers_preprocessed'] = df['answers'].map(lambda x: preprocess_2(x))\n",
    "\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=.1, random_state=RANDOM_SEED)\n",
    "# df_train, df_val = train_test_split(df_train, test_size=.25, random_state=RANDOM_SEED)\n",
    "    \n",
    "print(f\"Train Data \\t: {len(df_train)}\")\n",
    "# print(f\"Val Data \\t: {len(df_val)}\")\n",
    "print(f\"Test Data\\t: {len(df_test)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "british-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.LSTMBahdanau import Encoder, Decoder, Seq2Seq\n",
    "# from models.LSTMBahdanauImproved import Encoder, Decoder, Seq2Seq\n",
    "# from models.LSTMSelfAttn import Encoder, Decoder, Seq2Seq\n",
    "from models.LSTMBahdanauImprovedLM import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "changed-thermal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m test_data \u001b[38;5;241m=\u001b[39m MyData(input_tensor_test, target_tensor_test)\n\u001b[1;32m     22\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size \u001b[38;5;241m=\u001b[39m batch_size, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m encoder_net \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_embedding_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_word_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m decoder_net \u001b[38;5;241m=\u001b[39m Decoder(input_size_decoder, decoder_embedding_size, hidden_size, \n\u001b[1;32m     28\u001b[0m                       output_size, num_layers, dec_dropout, pretrained_word_embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, embedding_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, freeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m Seq2Seq(encoder_net, decoder_net, vocab_len\u001b[38;5;241m=\u001b[39mvocab_len)\n",
      "File \u001b[0;32m~/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/torch/nn/modules/rnn.py:194\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28;01mlambda\u001b[39;00m wn: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)(wn) \u001b[38;5;28;01mfor\u001b[39;00m wn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights_names]\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Flattens params (on CUDA)\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/torch/nn/modules/rnn.py:180\u001b[0m, in \u001b[0;36mRNNBase.flatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    179\u001b[0m     num_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 180\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cudnn_rnn_flatten_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cudnn_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "input_size_encoder = len(tokenizer.vocab)+4\n",
    "input_size_decoder = len(tokenizer.vocab)+4\n",
    "output_size = len(tokenizer.vocab)+4\n",
    "vocab_len = len(tokenizer.vocab)+4\n",
    "\n",
    "# encoder_embedding_size = pretrained_word_embedding_dimensions\n",
    "# decoder_embedding_size = pretrained_word_embedding_dimensions\n",
    "\n",
    "encoder_embedding_size = 384\n",
    "decoder_embedding_size = 384\n",
    "\n",
    "hidden_size = 768\n",
    "batch_size = 256\n",
    "num_layers = 1\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "input_tensor_test = df_test['questions_preprocessed'].values.tolist()\n",
    "target_tensor_test = df_test['answers_preprocessed'].values.tolist()\n",
    "\n",
    "test_data = MyData(input_tensor_test, target_tensor_test)\n",
    "test_dataset = DataLoader(test_data, batch_size = batch_size, drop_last=True, shuffle=True)\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, \n",
    "                  num_layers, enc_dropout, pretrained_word_embedding=False, embedding_matrix=None, freeze=False).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, \n",
    "                      output_size, num_layers, dec_dropout, pretrained_word_embedding=False, embedding_matrix=None, freeze=False).to(device)\n",
    "    \n",
    "model = Seq2Seq(encoder_net, decoder_net, vocab_len=vocab_len)\n",
    "# model.load_state_dict(torch.load(saved_model_path + \"/best_loss.pth\", map_location=device))\n",
    "model.load_state_dict(torch.load(saved_model_path + \"/model.pth\", map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = tokenizer.word2index[\"<PAD>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  test_batch_loss = 0\n",
    "  test_num_batch = 0\n",
    "  for (test_batch_idx, (X_test, y_test, input_len)) in enumerate(test_dataset):\n",
    "    X, y, input_lengths = sort_within_batch(X_test, y_test, input_len)\n",
    "\n",
    "    X = X.permute(1,0)\n",
    "    y = y.permute(1,0)\n",
    "\n",
    "    test_inp_data = X.to(device)\n",
    "    test_target = y.to(device)\n",
    "\n",
    "    output, _ = model(test_inp_data, test_target, input_lengths)\n",
    "#     output = model(test_inp_data, test_target, input_lengths)\n",
    "      \n",
    "    output = output[1:].reshape(-1, output.shape[2])\n",
    "    test_target = test_target[1:].reshape(-1)\n",
    "\n",
    "    test_loss = loss_function(test_target, output)\n",
    "    test_batch_loss += test_loss\n",
    "    test_num_batch+=1\n",
    "\n",
    "  test_loss_ = test_batch_loss/test_num_batch\n",
    "\n",
    "  print(f\"test_loss: {test_loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = df_test['questions'].values\n",
    "test_answers = df_test['answers'].values\n",
    "\n",
    "preds = []\n",
    "for x in test_questions:\n",
    "#     preds.append(respond_only_lstm_no_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "#     preds.append(respond_only_lstm_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "    # preds.append(respond_only_lstm_attn_reg(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "    preds.append(respond_only_lstm_attn_reg_lm(model, str(x), tokenizer, tokenizer, device, max_length=102))\n",
    "#     preds.append(respond_only_gru_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "#     preds.append(respond_only_gru_no_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bleu(preds, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bertscore(preds, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97924824-53b5-4812-a935-72665d567c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(sentence):\n",
    "  answer = respond_only_lstm_attn_reg_lm(model, str(sentence), tokenizer, tokenizer, device, max_length=52)\n",
    "  print('Me\\t:', sentence)\n",
    "  print('Bot\\t:', answer)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bb351-051b-4e22-a4a8-28d58cf99037",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond('hi')\n",
    "respond('good morning')\n",
    "respond('how are you ?')\n",
    "respond('Nice to meet you')\n",
    "respond('Where do you live now ?')\n",
    "respond('Good bye')\n",
    "respond(\"Do you like football ?\")\n",
    "respond(\"What is your favourite food ?\")\n",
    "respond(\"Do you know john ?\")\n",
    "respond(\"What is the matter ?\")\n",
    "respond(\"See you again\")\n",
    "respond(\"Who created you ?\")\n",
    "respond(\"It’s a pleasure to meet you\")\n",
    "respond(\"How’s it going ?\")\n",
    "respond(\"Can you help me ?\")\n",
    "respond(\"Hey\")\n",
    "respond(\"I need your help\")\n",
    "respond(\"Hello\")\n",
    "respond(\"Are you a bot ?\")\n",
    "respond(\"Hey Bot, how are you doing ?\")\n",
    "respond(\"Bot, can you come and help me ?\")\n",
    "respond(\"Would you please help me with my luggage ?\")\n",
    "respond(\"Do you need anything else ?\")\n",
    "respond(\"Ok, thank you\")\n",
    "respond(\"I am worried\")\n",
    "respond(\"Can we walk there ?\")\n",
    "respond(\"Ok, see you then, bye\")\n",
    "respond(\"I love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae76c0d-1e43-4d83-adfc-c27fbec684fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_chatbot",
   "language": "python",
   "name": "thesis_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
