{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "competent-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from random import seed, randrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sacrebleu\n",
    "import bert_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from models.LSTMBahdanau import Encoder, Decoder, Seq2Seq\n",
    "# from models.BiLSTMLuong import Encoder, Decoder, Seq2Seq\n",
    "from utils.tokenizer import Tokenizer, pad_sequences, respond\n",
    "from utils.tokenizer import respond_only_lstm_attn, respond_only_lstm_no_attn, respond_only_gru_no_attn, respond_only_gru_attn, respond_only_lstm_attn_reg, respond_only_lstm_attn_reg_lm, respond_only_lstm_attn_reg_lm_v2\n",
    "from utils.preprocess import preprocess_1, preprocess_2\n",
    "# from trainer import train, loss_function, sort_within_batch\n",
    "from utils.evaluate import calculate_rouge, calculate_bertscore, calculate_bleu\n",
    "\n",
    "root_dir = '/home/alfirsafauzulh@student.ub.ac.id/Firsa/Research/Chatbot'\n",
    "\n",
    "data_dir = root_dir + '/Datasets'\n",
    "dailydialogs_root_dir = data_dir + '/dailydialog'\n",
    "cornell_root_dir = data_dir + '/cornell_movie'\n",
    "# saved_model_path = '/home/alfirsafauzulh@student.ub.ac.id/Firsa/Research/Chatbot/Code/autogen-chatbot-v2/saved_models/siet/LSTMSA-dailydialog-50-SbertFreeze'\n",
    "# saved_model_path = '/home/alfirsafauzulh/Firsa/autogen-chatbot-v2/saved_models/siet/LSTMBahdanauLM-dailydialog-100-SbertFreeze'\n",
    "saved_model_path = '/home/alfirsafauzulh/Firsa/autogen-chatbot-v2/saved_models/siet/LSTMBahdanau-cornell-50-SbertFreeze'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sorted-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        # TODO: convert this into torch code is possible\n",
    "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        x_len = self.length[index]\n",
    "        return x,y,x_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)  \n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
    "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
    "    #print(mask)\n",
    "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    loss_ = criterion(pred, real) * mask \n",
    "    return torch.mean(loss_)\n",
    "\n",
    "### sort batch function to be able to use with pad_packed_sequence\n",
    "def sort_within_batch(X, y, lengths):\n",
    "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
    "    X = X[indx]\n",
    "    y = y[indx]\n",
    "    return X, y, lengths # transpose (batch x seq) to (seq x batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "opened-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "th = 50\n",
    "# df = pd.read_csv(dailydialogs_root_dir + f'/df_dailydialogs_max_{th}.csv')\n",
    "# df = pd.read_csv(f'./Datasets/dailydialog/df_dailydialogs_max_{th}.csv')\n",
    "# df = pd.read_csv(f'./Datasets/dailydialog/df_dailydialogs_factory.csv')\n",
    "df = pd.read_csv(f'./Datasets/cornell_movie/df_cornell_max_{th}.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reduced-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(saved_model_path + \"/tokenizer.pickle\", 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adapted-record",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data \t: 192769\n",
      "Test Data\t: 21419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = th+2\n",
    "\n",
    "df['questions_preprocessed'] = df['questions'].apply(preprocess_1)\n",
    "df['answers_preprocessed'] = df['answers'].apply(preprocess_1)\n",
    "\n",
    "df['questions_preprocessed'] = df['questions'].map(lambda x: preprocess_2(x))\n",
    "df['answers_preprocessed'] = df['answers'].map(lambda x: preprocess_2(x))\n",
    "\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['questions_preprocessed'] = df['questions_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: tokenizer.text_to_sequence(x))\n",
    "df['answers_preprocessed'] = df['answers_preprocessed'].map(lambda x: pad_sequences(x, max_len))\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=.1, random_state=RANDOM_SEED)\n",
    "# df_train, df_val = train_test_split(df_train, test_size=.25, random_state=RANDOM_SEED)\n",
    "    \n",
    "print(f\"Train Data \\t: {len(df_train)}\")\n",
    "# print(f\"Val Data \\t: {len(df_val)}\")\n",
    "print(f\"Test Data\\t: {len(df_test)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "british-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.LSTMBahdanau import Encoder, Decoder, Seq2Seq\n",
    "# from models.LSTMBahdanauImproved import Encoder, Decoder, Seq2Seq\n",
    "# from models.LSTMSelfAttn import Encoder, Decoder, Seq2Seq\n",
    "from models.LSTMBahdanauImprovedLM import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "changed-thermal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(45809, 384)\n",
       "    (rnn): LSTM(384, 768, dropout=0.5)\n",
       "    (lm_out): Linear(in_features=768, out_features=45809, bias=True)\n",
       "    (softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(45809, 384)\n",
       "    (rnn): LSTM(1152, 768, dropout=0.5)\n",
       "    (fc): Linear(in_features=768, out_features=45809, bias=True)\n",
       "    (W1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (W2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (V): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size_encoder = len(tokenizer.vocab)+4\n",
    "input_size_decoder = len(tokenizer.vocab)+4\n",
    "output_size = len(tokenizer.vocab)+4\n",
    "vocab_len = len(tokenizer.vocab)+4\n",
    "\n",
    "# encoder_embedding_size = pretrained_word_embedding_dimensions\n",
    "# decoder_embedding_size = pretrained_word_embedding_dimensions\n",
    "\n",
    "# encoder_embedding_size = 256\n",
    "# decoder_embedding_size = 256\n",
    "\n",
    "encoder_embedding_size = 384\n",
    "decoder_embedding_size = 384\n",
    "\n",
    "hidden_size = 768\n",
    "batch_size = 256\n",
    "num_layers = 1\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "input_tensor_test = df_test['questions_preprocessed'].values.tolist()\n",
    "target_tensor_test = df_test['answers_preprocessed'].values.tolist()\n",
    "\n",
    "test_data = MyData(input_tensor_test, target_tensor_test)\n",
    "test_dataset = DataLoader(test_data, batch_size = batch_size, drop_last=True, shuffle=True)\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, \n",
    "                  num_layers, enc_dropout, pretrained_word_embedding=False, embedding_matrix=None, freeze=False).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, \n",
    "                      output_size, num_layers, dec_dropout, pretrained_word_embedding=False, embedding_matrix=None, freeze=False).to(device)\n",
    "    \n",
    "model = Seq2Seq(encoder_net, decoder_net, vocab_len=vocab_len)\n",
    "# model.load_state_dict(torch.load(saved_model_path + \"/best_loss.pth\", map_location=device))\n",
    "model.load_state_dict(torch.load(saved_model_path + \"/model.pth\", map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "crucial-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.704239010810852\n"
     ]
    }
   ],
   "source": [
    "pad_idx = tokenizer.word2index[\"<PAD>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  test_batch_loss = 0\n",
    "  test_num_batch = 0\n",
    "  for (test_batch_idx, (X_test, y_test, input_len)) in enumerate(test_dataset):\n",
    "    X, y, input_lengths = sort_within_batch(X_test, y_test, input_len)\n",
    "\n",
    "    X = X.permute(1,0)\n",
    "    y = y.permute(1,0)\n",
    "\n",
    "    test_inp_data = X.to(device)\n",
    "    test_target = y.to(device)\n",
    "\n",
    "    output, _ = model(test_inp_data, test_target, input_lengths)\n",
    "#     output = model(test_inp_data, test_target, input_lengths)\n",
    "      \n",
    "    output = output[1:].reshape(-1, output.shape[2])\n",
    "    test_target = test_target[1:].reshape(-1)\n",
    "\n",
    "    test_loss = loss_function(test_target, output)\n",
    "    test_batch_loss += test_loss\n",
    "    test_num_batch+=1\n",
    "\n",
    "  test_loss_ = test_batch_loss/test_num_batch\n",
    "\n",
    "  print(f\"test_loss: {test_loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "miniature-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = df_test['questions'].values\n",
    "test_answers = df_test['answers'].values\n",
    "\n",
    "preds = []\n",
    "for x in test_questions:\n",
    "#     preds.append(respond_only_lstm_no_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "#     preds.append(respond_only_lstm_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "    # preds.append(respond_only_lstm_attn_reg(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "    preds.append(respond_only_lstm_attn_reg_lm(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "#     preds.append(respond_only_gru_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))\n",
    "#     preds.append(respond_only_gru_no_attn(model, str(x), tokenizer, tokenizer, device, max_length=52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "romantic-salem",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/alfirsafauzulh/.conda/envs/thesis_chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1-gram': 0.05515001536518298,\n",
       " '2-gram': 0.01143995313683888,\n",
       " '3-gram': 0.005732151082398569,\n",
       " '4-gram': 0.0037083484604785166}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bleu(preds, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fewer-celtic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 0.059088483452796936, 'r': 0.01952918991446495, 'f': 0.03834032639861107}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bertscore(preds, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97924824-53b5-4812-a935-72665d567c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(sentence):\n",
    "  answer = respond_only_lstm_attn_reg_lm(model, str(sentence), tokenizer, tokenizer, device, max_length=52)\n",
    "  print('Me\\t:', sentence)\n",
    "  print('Bot\\t:', answer)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bb351-051b-4e22-a4a8-28d58cf99037",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond('hi')\n",
    "respond('good morning')\n",
    "respond('how are you ?')\n",
    "respond('Nice to meet you')\n",
    "respond('Where do you live now ?')\n",
    "respond('Good bye')\n",
    "respond(\"Do you like football ?\")\n",
    "respond(\"What is your favourite food ?\")\n",
    "respond(\"Do you know john ?\")\n",
    "respond(\"What is the matter ?\")\n",
    "respond(\"See you again\")\n",
    "respond(\"Who created you ?\")\n",
    "respond(\"It’s a pleasure to meet you\")\n",
    "respond(\"How’s it going ?\")\n",
    "respond(\"Can you help me ?\")\n",
    "respond(\"Hey\")\n",
    "respond(\"I need your help\")\n",
    "respond(\"Hello\")\n",
    "respond(\"Are you a bot ?\")\n",
    "respond(\"Hey Bot, how are you doing ?\")\n",
    "respond(\"Bot, can you come and help me ?\")\n",
    "respond(\"Would you please help me with my luggage ?\")\n",
    "respond(\"Do you need anything else ?\")\n",
    "respond(\"Ok, thank you\")\n",
    "respond(\"I am worried\")\n",
    "respond(\"Can we walk there ?\")\n",
    "respond(\"Ok, see you then, bye\")\n",
    "respond(\"I love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae76c0d-1e43-4d83-adfc-c27fbec684fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_chatbot",
   "language": "python",
   "name": "thesis_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
